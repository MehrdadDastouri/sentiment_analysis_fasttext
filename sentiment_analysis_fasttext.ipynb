{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMw5NNSzi+F80LM+7YUG2UL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadDastouri/sentiment_analysis_fasttext/blob/main/sentiment_analysis_fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdPyGxrXIZe6",
        "outputId": "597f5deb-d9bc-45f0-f696-a937ce37a5c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4296187 sha256=35efad215a9383dece96c335872fa012d9e0f50601ad8b23d2ea1483ce4417f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load sample dataset for sentiment analysis\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product, it is amazing!\",\n",
        "        \"This is the worst experience I have ever had.\",\n",
        "        \"Absolutely fantastic! Highly recommend it.\",\n",
        "        \"I hate this. It is terrible.\",\n",
        "        \"Not bad, but could be better.\",\n",
        "        \"Worst service ever. Will never come back.\",\n",
        "        \"I really like it, great job!\",\n",
        "        \"Terrible experience, very disappointed.\",\n",
        "        \"I am so happy with this purchase!\",\n",
        "        \"This is okay, nothing special.\",\n",
        "        \"I am extremely unhappy with the service.\",\n",
        "    ],\n",
        "    \"label\": [\"__label__positive\", \"__label__negative\", \"__label__positive\", \"__label__negative\",\n",
        "              \"__label__neutral\", \"__label__negative\", \"__label__positive\", \"__label__negative\",\n",
        "              \"__label__positive\", \"__label__neutral\", \"__label__negative\"],\n",
        "}\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save training and test data to text files (required for FastText)\n",
        "train_data[[\"label\", \"text\"]].to_csv(\"train.txt\", index=False, sep=\" \", header=False, quoting=3, escapechar=\"\\\\\")\n",
        "test_data[[\"label\", \"text\"]].to_csv(\"test.txt\", index=False, sep=\" \", header=False, quoting=3, escapechar=\"\\\\\")\n",
        "\n",
        "# Train the FastText model\n",
        "print(\"Training FastText model...\")\n",
        "model = fasttext.train_supervised(input=\"train.txt\", epoch=25, lr=1.0, wordNgrams=2, verbose=2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"\\nEvaluating the model...\")\n",
        "test_results = model.test(\"test.txt\")\n",
        "print(f\"Number of examples: {test_results[0]}\")\n",
        "print(f\"Precision: {test_results[1]}\")\n",
        "print(f\"Recall: {test_results[2]}\")\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = []\n",
        "true_labels = []\n",
        "for index, row in test_data.iterrows():\n",
        "    pred_label = model.predict(row[\"text\"])[0][0]\n",
        "    predictions.append(pred_label)\n",
        "    true_labels.append(row[\"label\"])\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predictions, target_names=[\"positive\", \"neutral\", \"negative\"]))\n",
        "\n",
        "# Test with custom inputs\n",
        "print(\"\\nTesting with custom inputs...\")\n",
        "custom_texts = [\n",
        "    \"This product is amazing, I love it!\",\n",
        "    \"I am very disappointed with this service.\",\n",
        "    \"It's okay, nothing special but not bad either.\",\n",
        "    \"Worst experience ever, I will never buy this again.\",\n",
        "    \"Fantastic! Exceeded my expectations.\",\n",
        "]\n",
        "\n",
        "for text in custom_texts:\n",
        "    prediction = model.predict(text)[0][0]\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Sentiment: {prediction}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PMkTfpwH9-s",
        "outputId": "17dc75fd-883d-462d-b4ca-f881cc3195a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training FastText model...\n",
            "\n",
            "Evaluating the model...\n",
            "Number of examples: 3\n",
            "Precision: 0.3333333333333333\n",
            "Recall: 0.3333333333333333\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.33      1.00      0.50         1\n",
            "     neutral       0.00      0.00      0.00         1\n",
            "    negative       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.11      0.33      0.17         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "\n",
            "Testing with custom inputs...\n",
            "Text: This product is amazing, I love it!\n",
            "Predicted Sentiment: __label__negative\n",
            "\n",
            "Text: I am very disappointed with this service.\n",
            "Predicted Sentiment: __label__negative\n",
            "\n",
            "Text: It's okay, nothing special but not bad either.\n",
            "Predicted Sentiment: __label__negative\n",
            "\n",
            "Text: Worst experience ever, I will never buy this again.\n",
            "Predicted Sentiment: __label__negative\n",
            "\n",
            "Text: Fantastic! Exceeded my expectations.\n",
            "Predicted Sentiment: __label__negative\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}